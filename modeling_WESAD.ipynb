{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPc+x+w2c6sZKS0Y7qA/MBh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjspring/stress-detection-wearable-devices/blob/main/modeling_WESAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtX5TSM8gfZB",
        "outputId": "4d2a4d84-057a-47f2-e9c6-280b5402e0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Load pickled data\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "\n",
        "#! cd /content/drive/MyDrive/stress-prediction/ # Main file directory for this notebook on Google Drive\n",
        "os.chdir('/content/drive/MyDrive/stress-prediction')\n",
        "# absolute path of data directory\n",
        "PATH = os.path.join(os.path.abspath(os.getcwd()), 'data') # Path of data folder on Google Drive\n",
        "\n",
        "import joblib\n",
        "data = joblib.load(f\"{PATH}/pickle/WESAD_data_model.pickle\") # read pickle file\n",
        "labels = joblib.load(f\"{PATH}/pickle/WESAD_labels_model.pickle\") # read pickle file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsample the data and features\n",
        "subsample_rate = 175  # subsample rate, e.g. to reduce from 700Hz to 4Hz\n",
        "\n",
        "subsampled_data = data[::subsample_rate]\n",
        "subsampled_labels = labels[::subsample_rate]\n",
        "\n",
        "print(len(data))  # 2742499\n",
        "print(len(subsampled_data))  # 15672"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx2IGBv1yaQ7",
        "outputId": "96693d39-a832-4899-f303-14d8d80b1d41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2742499\n",
            "15672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try with a Datagenerator to reduce the RAM use\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "random_state = 42\n",
        "Hz = 4\n",
        "sampling_rate = 5 # keep one data point out of 5\n",
        "duration = 1 # how many minutes in the future the target after the end of the sequence\n",
        "sequence_length = Hz * 60 * duration # observations will go back duration minutues\n",
        "delay = sampling_rate*(sequence_length + duration*60*Hz - 1) # the target for a\n",
        "                                                             # sequence will be\n",
        "                                                             # duration (min)\n",
        "                                                             # after the end of\n",
        "                                                             # the sequence\n",
        "batch_size = 1\n",
        "shuffle = True\n",
        "\n",
        "# Train Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_dat, X_val, y_dat, y_val = train_test_split(subsampled_data, subsampled_labels, \n",
        "                                                 test_size = 0.2,\n",
        "                                                 random_state=random_state)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dat, y_dat,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = random_state)\n",
        "\n",
        "train_data_gen = TimeseriesGenerator(X_train, y_train, length=sequence_length, batch_size=batch_size)\n",
        "val_data_gen = TimeseriesGenerator(X_val, y_val, length=sequence_length, batch_size=batch_size)\n",
        "test_data_gen = TimeseriesGenerator(X_test, y_test, length=sequence_length, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "3xkosnn-j7z6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(64, \n",
        "               activation='relu', \n",
        "               stateful = True, # To save RAM use\n",
        "               batch_input_shape=(batch_size, sequence_length, 1)))\n",
        "              # input_shape=(sequence_length, 1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['binary_accuracy'])\n",
        "\n",
        "# Train the model using the TimeSeriesGenerator\n",
        "# train the model\n",
        "epochs = 10\n",
        "total_epochs = 0\n",
        "for i in range(epochs):\n",
        "    for j in range(len(train_data_gen)):\n",
        "      print('Epoch', total_epochs+1, '/', epochs)\n",
        "      model.fit(train_data_gen, epochs=1, validation_data=val_data_gen, verbose=1, shuffle=False)\n",
        "      model.reset_states() # Need to reset since using LSTM stateful\n",
        "      total_epochs += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D6BD7Zoheo2",
        "outputId": "1e318aac-0104-44f8-f2e3-34d98595cc78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 10\n",
            "9789/9789 [==============================] - 984s 100ms/step - loss: nan - binary_accuracy: 0.8142 - val_loss: nan - val_binary_accuracy: 0.8138\n",
            "Epoch 1 / 10\n",
            "4513/9789 [============>.................] - ETA: 8:07 - loss: nan - binary_accuracy: 0.8043"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation and test dataset\n",
        "\n",
        "score = model.evaluate(val_data_gen)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "\n",
        "test_loss = model.evaluate(test_data_gen)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "btVwVtufhits"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}